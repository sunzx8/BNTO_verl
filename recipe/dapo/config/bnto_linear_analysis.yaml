hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

data:
  gen_batch_size: ${data.train_batch_size}

reward_model:
  reward_manager: dapo
  overlong_buffer: 
    enable: False
    len: 0
    penalty_factor: 0.0
    log: False

algorithm:
  filter_groups:
    _target_: verl.trainer.config.FilterGroupsConfig
    enable: False
    metric: null
    max_num_gen_batches: 0
  # BNTO specific: use dual_game loss
  policy_loss:
    loss_mode: dual_game
    dual_game:
      gamma: 1.5  # 放大负优势，与论文一致
      lambda_coef: 0.0  # 由controller动态更新
      beta_coef: 0.0    # 由controller动态更新

# 熵预算控制 (仅线性衰减，基于sigma_R)
entropy_budget:
  target: 5.0
  lambda_init: 0.0
  lambda_lr: 0.05
  budget_mix_alpha: 1.0  # 仅使用sigma_R，不混合usage_ratio
  adaptive_enabled: false

# KL控制
critic:
  kl_ctrl:
    type: linear
    beta_init: 0.0
    beta_lr: 0.01
    target_kl: 0.04
    adaptive_enabled: false

trainer:
  project_name: verl-bnto-linear-analysis 